{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsers para Gramáticas independientes de contexto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK\n",
    "\n",
    "Natural Language Toolkit es un conjunto de herramientas para crear programas en python que trabajen con lenguaje natural. nltk.org (la organización a cargo de crear y mantener esta librería) pone a disposición de manera online el [NLTK Book](https://www.nltk.org/book/), su libro especializado en el uso de la librería así como la explicación de conceptos generales de PLN.\n",
    "\n",
    "Para estar al día con los cambios en el código, lo mejor es chequear este libro on-line en vez de se versión editada, que puede traer ejemplos deprecados (\"nbest_parse\" vs. \"parse\")\n",
    "\n",
    "\n",
    "Al instalarnos esta librería, podemos acceder y hacer uso de sus funciones y clases para ayudar a construir nuestra propia herramienta.\n",
    "\n",
    "Por ejemplo, si quisieramos crear un pequeño corrector gramatical, podríamos utilizar alguno de los parsers de gramáticas independientes de contexto para identificar oraciones no gramaticales y devolver una alerta en este caso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gramáticas para NLTK\n",
    "\n",
    "Antes de pasar a los parsers, miremos las gramáticas que vamos a \"parsear\", construidas según lo pide el libro de Bird et al.\n",
    "[LINK A GRAMATICA](CFG.cfg)\n",
    "Notemos que la extensión del archivo es \".cfg\", así le vamos a avisar a nltk que la gramática debe ser entendida como \"Context Free Grammar\".\n",
    "Ahora podemos mirar por adentro que la grámatica cuenta con todos los elementos que, según lo que vimos en la clase, constituyen el formalismo de una CFG:\n",
    "\n",
    "Un axioma: O\n",
    "Símbolos no terminales: SN, PRO, NP, etc.\n",
    "Símbolos terminales: martín, cata, etc.\n",
    "Reglas de reescritura: cada una de las líneas de la gramática, que deben indicar que un elemento a la izquierda del signo -> se debe reescribir como los elementos a la derecha."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora sí, accedamos a los distintos parsers y al finalizar podemos concretar el pequeño corrector gramatical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recursive Descent Parser\n",
    "\n",
    "Este parser es de tipo **top-down** (analiza de arriba hacia abajo). Es decir que parte del símbolo de inicio y aplica las reglas de la gramática para obtener los constituyentes inmediatos y armar el árbol hasta llegar a los símbolos terminales. \n",
    "\n",
    "Debe chequear que los símbolos terminales coincidan con la secuencia del input sin haberla visto de antemano. Si no hay coincidencia, tiene que retroceder y buscar diferentes alternativas de parseo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pasemos al parser:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recursive Descent Parser\n",
    "\n",
    "def rd_parser(oracion, gramatica):                  # Definimos una función llamada rd_parser con dos argumentos.\n",
    "    oracion = oracion.lower()                       # Convertimos a minúscula la oración utilizando una función nativa de la cadena de caracteres: lower(). \n",
    "        \n",
    "    if oracion.endswith('.'):                       # Otra función nativa de las strings nos ayuda a chequear si la cadena termina en x argumento.\n",
    "        oracion = re.sub('\\.',' ',oracion)          # En este caso, si la oración termina con un punto, se lo quita utilizando la librería de expresiones regulares \"re\".\n",
    "    else:                                           # Si no termina con un punto, \n",
    "        oracion = oracion                           # toma la oración como estaba originalmente.\n",
    "    lista_palabras = oracion.split()              # Dividimos la oración en palabras tomando como separador el espacio en blanco  con otra función nativa de las strings: split.\n",
    "    print(\"- Esta es la lista de palabras resultante: \", lista_palabras) # Split nos devuelve una lista (ordenada) de strings.\n",
    "      \n",
    "    gramatica = nltk.data.load(gramatica)           # Usamos la función de la sub librería \"data\" que nos permite cargar una gramática para que pueda ser usada luego por el parser.    \n",
    "    rd_parser = nltk.RecursiveDescentParser(gramatica) # Instanciamos la clase del parser que nos da NLTK pasandole un argumento obligatorio: la gramática.\n",
    "    for arbol in rd_parser.parse(lista_palabras):    # Una vez que instanciamos la clase, podemos usar sus funciones mientras le pasemos los argumentos requeridos. En este caso, usamos la función \"parser\" a la que le pasaremos nuestra lista de palabras, y la función nos devolverá cada árbol posible en mi gramática para esa oración.\n",
    "        print(\"- Este es el árbol resultante: \", arbol) # Imprimimos cada árbol en la consola."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Escribí una oración:\n",
      "\n",
      "- Esta es la lista de palabras resultante:  []\n"
     ]
    }
   ],
   "source": [
    "#Para correr el Recursive Descent Parser\n",
    "\n",
    "print('Escribí una oración:')                          # Para que me pida que escriba una oración\n",
    "oracion1 = input()                                     # Para que me abra un campo en el que escriba la oración\n",
    "gramatica = 'gramaticas/CFG.cfg'                       # Indicamos el path a nuestra gramatica\n",
    "rd_parser(oracion1, gramatica)                         # Llamamos a la función que creamos con los dos argumentos que establecimos como obligatorios.\n",
    "\n",
    "# Oraciones que acepta la gramática: \n",
    "# Cata/Martín/Julia/Maca/Pablo fuma\n",
    "# Cata/Martín/Julia/Maca/Pablo entregó/envió el/la/un/una plaza/facultad/regalo/globo/tabaco"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algunas limitaciones del recursive descent parser\n",
    "\n",
    "- 1. La recursión a la izquierda provoca un loop infinito. (SN -> PRO | SN NP)\n",
    "- 2. El parser puede llegar a tomar demasiado tiempo en considerar opciones que mirando la oración ya sabemos que no son posibles. (Fernando fuma)\n",
    "- 3. El movimiento de backtracking borra construcciones de consituyentes que podrían ser útiles para otras partes de la oración. (El cigarrillo fue fumado por la persona)\n",
    "\n",
    "Veamos todo esto en la demo:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Demo del Right Descent Parser**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.app.rdparser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shift Reduce Parser\n",
    "\n",
    "Este parser, en cambio, es **botom-up**, es decir que parte de la secuencia de palabras que conforman la oración a parsear y busca asignarle una estructura acorde con la gramática. Es decir que busca secuencias de palabras que coincidan con el lado derecho de las producciones de la gramática para reemplazarlas por el símbolo del lado izquierdo.\n",
    "\n",
    "Por ejemplo, si encuentra la secuencia \"fuma\" y en la gramática posee la regla V -> \"fuma\", hará el reemplazo por el símbolo V.\n",
    "\n",
    "Ahora bien, el parser intentará que la subsecuencia más larga posible coincida con los símbolos a la derecha, para ello usa un \"stack\" (una pila, donde se apilan cosas), una especie de memoria temporal donde acumula palabras de una secuencia, una a una, mientras intenta hacerlas coincidir con el lado derecho de una producción. Esta es la acción de \"shift\" (desplazamiento).\n",
    "\n",
    "Una vez que la subsecuencia coincide con una de las producciones, la reemplaza por el símbolo del lado izquierdo. Esta es la acción de \"reduce\" (reducir).\n",
    "\n",
    "\n",
    "El parser aplicará estos pasos hasta alcanzar el símbolo del axioma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shift Reduce Parser\n",
    "\n",
    "def sr_parser(oracion, gramatica):                   # Definimos una función llamada sr_parser con dos argumentos.\n",
    "    oracion = oracion.lower()\n",
    "    if oracion.endswith('.'):\n",
    "        oracion = re.sub('\\.',' ',oracion)\n",
    "    else:\n",
    "        oracion = oracion\n",
    "    lista_palabras = oracion.split()\n",
    "    gramatica = nltk.data.load(gramatica)\n",
    "    sr_parser = nltk.ShiftReduceParser(gramatica)    # Instanciamos otra clase de parser\n",
    "    for arbol in sr_parser.parse(lista_palabras):\n",
    "        print(\"- Este es el árbol resultante: \", arbol)\n",
    "        #return(arbol)                                # Hacemos un retorno para la función, es decir que la función aquí se va a terminar, lo que nos cortara el loop, pero nos dibujará el árbol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Escribí una oración:')\n",
    "oracion2 = input()\n",
    "gramatica = 'gramaticas/CFG.cfg'\n",
    "sr_parser(oracion2, gramatica)   \n",
    "\n",
    "# Oraciones que acepta la gramática: \n",
    "# Cata/Martín/Julia/Maca/Pablo fuma\n",
    "# Cata/Martín/Julia/Maca/Pablo entregó/envió el/la/un/una plaza/facultad/regalo/globo/tabaco"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pero qué son esos Warnings?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algunas limitaciones del shift reduce parser\n",
    "\n",
    "- 1. Solo puede devolver un árbol posible, aunque la oración sea ambigua y acepte más de una estructura.\n",
    "- 2. En cada acción de reducir, debe seleccionar una, aunque haya más de una posible. Y si la posibilidad de hacer shift o reduce es ambivalente, deberá decidir por una de las dos acciones. Fallas en estas decisiones pueden resultar en una falla del parseo y, al no tener implementada una forma de backtracking, si siguió un camino que fue infructuoso, decidirá que esa oración no tiene solución. (Fernando fuma el cigarrillo en el parque)\n",
    "\n",
    "\n",
    "Veamos todo esto en la demo:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Demo del Shift and Reduce parser**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.app.srparser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chart Parser\n",
    "\n",
    "Los parsers que vimos hasta acá tienen deficiencias, sea en eficiencia o completitud. El chart parser usa dynamic programming para ser más eficiente. Dynamic programming es una técnica para desarrollar algoritmos que tiende a solucionar un problema subdividiendolo en sub problemas. Consiste en guardar la solución a esos sub problemas para poder reusarla cada vez que se la necesita.\n",
    "\n",
    "El chart parser aplica esta técnica. Por ejemplo, construirá el SP \"con el telescopio\" una vez y lo guardará en una tabla. Esta tabla se denomina WFST (tabla de subcadenas bien formadas).\n",
    "\n",
    "Vamos a armar una:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_wfst(tokens, grammar):    \n",
    "    numtokens = len(tokens)    \n",
    "    wfst = [[None for i in range(numtokens+1)] for j in range(numtokens+1)]    # Esta forma de escribir un loop se llama \"list comprehension\"\n",
    "    for i in range(numtokens):        \n",
    "        productions = grammar.productions(rhs=tokens[i])        \n",
    "        wfst[i][i+1] = productions[0].lhs()    \n",
    "    return wfst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_wfst(wfst, tokens, grammar, trace=False):    \n",
    "    index = dict((p.rhs(), p.lhs()) for p in grammar.productions())    \n",
    "    numtokens = len(tokens)    \n",
    "    for span in range(2, numtokens+1):        \n",
    "        for start in range(numtokens+1-span):           \n",
    "            end = start + span            \n",
    "            for mid in range(start+1, end):                \n",
    "                nt1, nt2 = wfst[start][mid], wfst[mid][end]                \n",
    "                if nt1 and nt2 and (nt1,nt2) in index:                    \n",
    "                    wfst[start][end] = index[(nt1,nt2)]                    \n",
    "                    if trace:                        \n",
    "                        print(\"[%s] %3s [%s] %3s [%s] ==> [%s] %3s [%s]\" % (start, nt1, mid, nt2, end, start, index[(nt1,nt2)], end))    \n",
    "    return wfst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display(wfst, tokens):    \n",
    "    print('\\nWFST ' + ' '.join([(\"%-4d\" % i) for i in range(1, len(wfst))]))    \n",
    "    for i in range(len(wfst)-1):        \n",
    "        print(\"%d   \" % i, end=' ')        \n",
    "        for j in range(1, len(wfst)):            \n",
    "            print(\"%-4s\" % (wfst[i][j] or '.'), end=' ')        \n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oracion = \"fernando fuma en el balcon\".split()\n",
    "for indice in range(len(oracion)):\n",
    "    print(indice, oracion[indice])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart_gramatica = nltk.CFG.fromstring(\n",
    "    \"\"\"O -> SN SV\n",
    "    SP -> P SN\n",
    "    SN -> Det NC | 'fernando'\n",
    "    SV -> V NP | V SP\n",
    "    Det -> 'el'\n",
    "    NC -> 'balcon'\n",
    "    V -> 'fuma'\n",
    "    P -> 'en'\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wfst0 = init_wfst(oracion, chart_gramatica)\n",
    "display(wfst0, oracion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wfst1 = complete_wfst(wfst0, oracion, chart_gramatica, trace=True)\n",
    "display(wfst1, oracion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos el resultado de correr el Chart Parser por una oración con nuestra gramática original:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grammar with 46 productions (start state = S)\n",
      "    S -> SN SV\n",
      "    SN -> PRO\n",
      "    SN -> 'martín'\n",
      "    SN -> 'cata'\n",
      "    SN -> 'fernando'\n",
      "    SN -> 'fede'\n",
      "    SN -> 'maca'\n",
      "    SN -> 'pablo'\n",
      "    SN -> D NC\n",
      "    SN -> D NC SP\n",
      "    NC -> 'plaza'\n",
      "    NC -> 'facultad'\n",
      "    NC -> 'regalo'\n",
      "    NC -> 'globo'\n",
      "    NC -> 'tabaco'\n",
      "    NC -> 'persona'\n",
      "    NC -> 'cigarrillo'\n",
      "    NC -> 'telescopio'\n",
      "    D -> 'el'\n",
      "    D -> 'la'\n",
      "    D -> 'un'\n",
      "    D -> 'una'\n",
      "    PRO -> 'ella'\n",
      "    PRO -> 'él'\n",
      "    SV -> FV SN SP\n",
      "    SV -> FV SP SP\n",
      "    SV -> FV SP\n",
      "    SV -> FV\n",
      "    SV -> FV SN\n",
      "    FV -> AUX PART\n",
      "    FV -> V\n",
      "    AUX -> 'fue'\n",
      "    PART -> 'entregado'\n",
      "    PART -> 'enviado'\n",
      "    PART -> 'fumado'\n",
      "    PART -> 'explotado'\n",
      "    V -> 'entregó'\n",
      "    V -> 'envió'\n",
      "    V -> 'explotó'\n",
      "    V -> 'fuma'\n",
      "    V -> 've'\n",
      "    SP -> P SN\n",
      "    P -> 'por'\n",
      "    P -> 'en'\n",
      "    P -> 'a'\n",
      "    P -> 'con'\n"
     ]
    }
   ],
   "source": [
    "gramatica = 'gramaticas/CFG.cfg'\n",
    "gramatica = nltk.data.load(gramatica)\n",
    "print(gramatica)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S (SN fernando) (SV (FV (V fuma))))\n",
      "(SN fernando)\n",
      "(SV (FV (V fuma)))\n",
      "(FV (V fuma))\n",
      "(V fuma)\n"
     ]
    }
   ],
   "source": [
    "parser = nltk.ChartParser(gramatica)\n",
    "for tree in parser.parse(['fernando', 'fuma']):\n",
    "    #print(tree.draw()) # La notación de punto nos permite acceder a métodos del objeto. Descomenten las lineas con \"print\" y miren qué hace cada método.\n",
    "    #print(tree.flatten()) \n",
    "    #print(tree.productions())\n",
    "    #print(tree.) #Descomenten la línea y usen la tecla \"tab\" para ver qué otros métodos ofrece el objeto.\n",
    "    for st in tree.subtrees():\n",
    "        print(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Demo para el Chart Parser\n",
    "#nltk.app.chartparser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BLLIP Parser\n",
    "\n",
    "Brown Laboratory for Linguistic Information Processing\n",
    "\n",
    "\n",
    "Bllip parser es un \"reranking parser\", es decir, un parser que va a devolver una serie de posibles árboles para una oración, ordenados según su probabilidad del más probable al menos probable; y una vez que obtuvo los 50 mejores árboles, va a aplicar otra estrategia de ranking para reordenar (rerank) este subset de resultados consiguiendo incluso mayor precisión.\n",
    "\n",
    "La probabilidad de un determinado resultado viene dada por el modelo de lenguaje que el parser usa. El modelo provisto por BLLIP fue entrenado con un corpus de árboles en inglés (El Penn TreeBank) pero sus resultados podrían variar dependiendo de los datos usados en el entrenamiento, así como del método utilizado para entrenar. \n",
    "\n",
    "[Eugene Charniak. \"A maximum-entropy-inspired parser.\" Proceedings of the 1st North American chapter of the Association for Computational Linguistics conference. Association for Computational Linguistics, 2000.](https://aclanthology.org/A00-2018.pdf)\n",
    "\n",
    "\n",
    "[Penn TreeBank](https://catalog.ldc.upenn.edu/LDC99T42)\n",
    "\n",
    "\n",
    "[TreeBank Wikipedia](https://es.wikipedia.org/wiki/TreeBank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install --user bllipparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import bllipparser\n",
    "from bllipparser import RerankingParser                             #Importa el parser\n",
    "from bllipparser.ModelFetcher import download_and_install_model     # Descarga e instala el \"modelo\"\n",
    "\n",
    "model_dir = download_and_install_model('WSJ', 'tmp/models')         #Crea una variable con el \"modelo\"\n",
    "rrp = RerankingParser.from_unified_model_dir(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(S1 (S (NP (NNP john)) (VP (VBZ runs) (PP (IN through) (NP (DT the) (NN hill))))))'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oracion2 = \"john runs through the hill\"\n",
    "rrp.simple_parse(oracion2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-61.473407804090 -15.666584843061 (S1 (S (NP (NNP john)) (VP (VBZ runs) (PP (IN through) (NP (DT the) (NN hill))))))\n",
      "-70.113439455262 -18.636868101519 (S1 (S (NP (NNP john)) (VP (VBZ runs) (PP (IN through) (NP (DT the) (NNP hill))))))\n",
      "-68.382454287091 -18.998642332884 (S1 (S (NP (NNP john)) (VP (NNS runs) (PP (IN through) (NP (DT the) (NN hill))))))\n",
      "-67.410672470591 -19.181059117030 (S1 (NP (NP (NNP john)) (VP (VBZ runs) (PP (IN through) (NP (DT the) (NN hill))))))\n",
      "-69.418171102276 -20.365130424824 (S1 (NP (NP (NNP john) (VBZ runs)) (PP (IN through) (NP (DT the) (NN hill)))))\n",
      "-72.496708134442 -20.572325714804 (S1 (NP (NP (NNP john) (NNS runs)) (PP (IN through) (NP (DT the) (NN hill)))))\n",
      "-69.159051872109 -20.809747309476 (S1 (S (NP (NNP john) (NNS runs)) (PP (IN through) (NP (DT the) (NN hill)))))\n",
      "-72.853753339970 -21.228074880022 (S1 (S (NP (NNP john)) (VP (VBZ runs) (PP (RP through) (NP (DT the) (NN hill))))))\n",
      "-76.156101753673 -21.335983455203 (S1 (S (NP (NNP john) (NNS runs)) (VP (PP (IN through) (NP (DT the) (NN hill))))))\n",
      "-74.662999515759 -21.451079292808 (S1 (NP (NNP john) (VBZ runs) (PP (IN through) (NP (DT the) (NN hill)))))\n",
      "-74.929389026579 -21.615482629952 (S1 (S (NP (NNP john)) (VP (VBZ runs) (PRT (RP through)) (NP (DT the) (NN hill)))))\n",
      "-73.786396171009 -21.755007416103 (S1 (S (NP (NP (NNP john) (NNS runs)) (PP (IN through) (NP (DT the) (NN hill))))))\n",
      "-72.946206104644 -21.779601357204 (S1 (S (NP (NNP john) (VBZ runs)) (PP (IN through) (NP (DT the) (NN hill)))))\n",
      "-74.582761842937 -21.844278765456 (S1 (S (NP (NNP john)) (VP (VBZ runs)) (PP (IN through) (NP (DT the) (NN hill)))))\n",
      "-77.022485938263 -21.867052858419 (S1 (S (NP (NNP john)) (VP (NNS runs) (PP (IN through) (NP (DT the) (NNP hill))))))\n",
      "-77.325509068846 -21.878272006251 (S1 (S (NP (NNP john)) (VP (VBZ runs) (VP (PP (IN through) (NP (DT the) (NN hill)))))))\n",
      "-77.510279033428 -22.035874646592 (S1 (S (NP (NNP john)) (VP (VBZ runs) (ADVP (RB through)) (NP (DT the) (NN hill)))))\n",
      "-76.050704121763 -22.074858952565 (S1 (NP (NP (NNP john)) (VP (VBZ runs) (PP (IN through) (NP (DT the) (NNP hill))))))\n",
      "-80.773178898354 -22.243750069172 (S1 (S (NP (NNP john)) (VP (VP (VBZ runs)) (PP (IN through) (NP (DT the) (NN hill))))))\n",
      "-73.828278984451 -22.323632011278 (S1 (S (NP (NP (NNP john) (VBZ runs)) (PP (IN through) (NP (DT the) (NN hill))))))\n",
      "-77.091628358056 -22.581085486455 (S1 (S (NP (NP (NNP john)) (VP (VBZ runs) (PP (IN through) (NP (DT the) (NN hill)))))))\n",
      "-74.428234892074 -23.064278966830 (S1 (S (NP (NNP john)) (VP (VBZ runs) (PP (RB through) (NP (DT the) (NN hill))))))\n",
      "-77.425474347380 -23.294309854521 (S1 (NP (NP (NNP john) (VBZ runs)) (PP (IN through) (NP (DT the) (NNP hill)))))\n",
      "-80.504011379546 -23.333923804501 (S1 (NP (NP (NNP john) (NNS runs)) (PP (IN through) (NP (DT the) (NNP hill)))))\n",
      "-79.055131423491 -23.354453604820 (S1 (S (NP (NNP john)) (VP (VBZ runs) (ADVP (IN through)) (NP (DT the) (NN hill)))))\n",
      "-77.705251364669 -23.399005947403 (S1 (S (NP (NNP john)) (VP (VBZ runs) (PP (IN through) (S (NP (DT the) (NN hill)))))))\n",
      "-78.482359919830 -23.469333122878 (S1 (NP (NP (NNP john)) (VP (NNS runs) (PP (IN through) (NP (DT the) (NN hill))))))\n",
      "-80.062712748343 -23.470737010839 (S1 (S (NP (NNP john) (NNS runs)) (VP (IN through) (NP (DT the) (NN hill)))))\n",
      "-78.782866665644 -23.562304384649 (S1 (S (NP (NNP john)) (VP (VBZ runs) (NP (IN through) (DT the) (NN hill)))))\n",
      "-77.491453134259 -23.742847165144 (S1 (S (NP (NNP john) (NNS runs)) (PP (IN through) (NP (DT the) (NNP hill)))))\n",
      "-80.667240569154 -23.834006490657 (S1 (S (NP (NNP john)) (VP (VBZ runs) (ADVP (RP through)) (NP (DT the) (NN hill)))))\n",
      "-79.599280191154 -23.890858933720 (S1 (S (NP (NNP john)) (VP (VBZ runs) (PRT (IN through)) (NP (DT the) (NN hill)))))\n",
      "-79.813899587185 -23.899108897287 (S1 (S (NP (NNP john)) (VP (VBZ runs) (NP (RB through) (DT the) (NN hill)))))\n",
      "-78.804575134954 -23.930016088884 (S1 (NP (NP (NNP john)) (VP (VBZ runs)) (PP (IN through) (NP (DT the) (NN hill)))))\n",
      "-78.855847128776 -24.138990673588 (S1 (S (NP (NNP john)) (VP (NNS runs) (PRT (RP through)) (NP (DT the) (NN hill)))))\n",
      "-81.793699416113 -24.279880275800 (S1 (S (NP (NP (NNP john) (NNS runs)) (PP (IN through) (NP (DT the) (NNP hill))))))\n",
      "-81.231388714607 -24.424882238424 (S1 (S (NP (NNP john)) (VP (VBZ runs) (RB through) (NP (DT the) (NN hill)))))\n",
      "-80.420420607420 -24.584549534796 (S1 (NP (NP (NNP john)) (VP (VBZ runs) (VP (PP (IN through) (NP (DT the) (NN hill)))))))\n",
      "-81.029163050303 -24.604300613090 (S1 (NP (NP (NP (NNP john)) (VP (VBZ runs))) (PP (IN through) (NP (DT the) (NN hill)))))\n",
      "-79.536476229337 -24.655956904420 (S1 (S (NP (NNP john)) (VP (NNS runs) (PP (RP through) (NP (DT the) (NN hill))))))\n",
      "-79.108441505987 -24.720113419685 (S1 (S (NP (NNP john)) (VP (VBZ runs) (ADVP (IN through) (NP (DT the) (NN hill))))))\n",
      "-78.869947976295 -24.846633299903 (S1 (NP (NP (NNP john)) (VP (VBZ runs) (PP (RP through) (NP (DT the) (NN hill))))))\n",
      "-81.278607366795 -24.880282555794 (S1 (S (NP (NNP john) (VBZ runs)) (PP (IN through) (NP (DT the) (NNP hill)))))\n",
      "-80.486055751547 -24.967117132487 (S1 (NP (NP (NNP john)) (VP (VBZ runs) (PRT (RP through)) (NP (DT the) (NN hill)))))\n",
      "-81.835582229555 -25.016086210975 (S1 (S (NP (NP (NNP john) (VBZ runs)) (PP (IN through) (NP (DT the) (NNP hill))))))\n",
      "-81.652190172533 -25.093127824227 (S1 (NP (NP (NNP john)) (VP (VBZ runs) (ADVP (RB through)) (NP (DT the) (NN hill)))))\n",
      "-81.458339498773 -26.164378464222 (S1 (S (NP (NNP john)) (VP (NNS runs) (PP (RB through) (NP (DT the) (NN hill))))))\n",
      "-81.639997084258 -26.222913181684 (S1 (S (NP (NNP john) (NNS runs)) (PP (RP through) (NP (DT the) (NN hill)))))\n",
      "-80.101712897705 -26.272459138080 (S1 (S (NP (NNP john) (NNS runs)) (PP (RB through) (NP (DT the) (NN hill)))))\n",
      "-80.679726153480 -26.747590441617 (S1 (NP (NP (NNP john)) (VP (VBZ runs) (PP (RB through) (NP (DT the) (NN hill))))))\n"
     ]
    }
   ],
   "source": [
    "for parse in rrp.parse(oracion2):\n",
    "    print(parse) #Probar los métodos de parse usando notación de . y \"tab\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('john', 'NNP'),\n",
       " ('runs', 'VBZ'),\n",
       " ('through', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('hill', 'NN')]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rrp.tag(oracion2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Escribí una oración en inglés')\n",
    "oracion4 = input()\n",
    "rrp.simple_parse(oracion4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Método tree - árboles con el formato del Penn TreeBank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "oracion3 = \"No one saw him disembark in the unanimous night, no one saw the bamboo canoe sink into the sacred mud, but in a few days there was no one who did not know that the taciturn man came from the South\"\n",
    "structure = rrp.simple_parse(oracion3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S1 (S (S (NP (DT No) (NN one)) (VP (VBD saw) (S (NP (PRP him)) (VP (VBP disembark) (PP (IN in) (NP (DT the) (JJ unanimous) (NN night))))))) (, ,) (S (NP (DT no) (NN one)) (VP (VBD saw) (S (NP (DT the) (NN bamboo) (NN canoe)) (VP (VB sink) (PP (IN into) (NP (DT the) (JJ sacred) (NN mud))))))) (, ,) (CC but) (S (PP (IN in) (NP (DT a) (JJ few) (NNS days))) (NP (EX there)) (VP (VBD was) (NP (NP (DT no) (NN one)) (SBAR (WHNP (WP who)) (S (VP (VBD did) (RB not) (VP (VB know) (SBAR (IN that) (S (NP (DT the) (JJ taciturn) (NN man)) (VP (VBD came) (PP (IN from) (NP (DT the) (NNP South)))))))))))))))\n",
      "(S1 (S (S (NP (DT No) (NN one))\n",
      "\t(VP (VBD saw)\n",
      "\t (S (NP (PRP him))\n",
      "\t  (VP (VBP disembark)\n",
      "\t   (PP (IN in) (NP (DT the) (JJ unanimous) (NN night)))))))\n",
      "     (, ,)\n",
      "     (S (NP (DT no) (NN one))\n",
      "      (VP (VBD saw)\n",
      "       (S (NP (DT the) (NN bamboo) (NN canoe))\n",
      "\t(VP (VB sink)\n",
      "\t (PP (IN into) (NP (DT the) (JJ sacred) (NN mud)))))))\n",
      "     (, ,)\n",
      "     (CC but)\n",
      "     (S (PP (IN in) (NP (DT a) (JJ few) (NNS days)))\n",
      "      (NP (EX there))\n",
      "      (VP (VBD was)\n",
      "       (NP (NP (DT no) (NN one))\n",
      "\t(SBAR (WHNP (WP who))\n",
      "\t (S (VP (VBD did)\n",
      "\t     (RB not)\n",
      "\t     (VP (VB know)\n",
      "\t      (SBAR (IN that)\n",
      "\t       (S (NP (DT the) (JJ taciturn) (NN man))\n",
      "\t\t(VP (VBD came)\n",
      "\t\t (PP (IN from) (NP (DT the) (NNP South)))))))))))))))\n",
      "S1\n",
      "(0, 43)\n"
     ]
    }
   ],
   "source": [
    "tree = bllipparser.Tree(structure)\n",
    "prettytree = tree.pretty_string()\n",
    "sentenceroot = tree.label\n",
    "sentencespan = tree.span()\n",
    "print(tree)\n",
    "print(prettytree)\n",
    "print(sentenceroot)\n",
    "print(sentencespan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treebank en NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package treebank to /home/grmf/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/treebank.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import treebank\n",
    "#nltk.download('treebank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP-SBJ\n",
      "    (NP (NNP Pierre) (NNP Vinken))\n",
      "    (, ,)\n",
      "    (ADJP (NP (CD 61) (NNS years)) (JJ old))\n",
      "    (, ,))\n",
      "  (VP\n",
      "    (MD will)\n",
      "    (VP\n",
      "      (VB join)\n",
      "      (NP (DT the) (NN board))\n",
      "      (PP-CLR (IN as) (NP (DT a) (JJ nonexecutive) (NN director)))\n",
      "      (NP-TMP (NNP Nov.) (CD 29))))\n",
      "  (. .))\n"
     ]
    }
   ],
   "source": [
    "t = treebank.parsed_sents('wsj_0001.mrg')[0] # Wall Street Journal\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
